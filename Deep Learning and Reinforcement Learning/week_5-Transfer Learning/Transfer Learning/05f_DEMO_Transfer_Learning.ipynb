{"cells":[{"cell_type":"markdown","metadata":{},"source":["<p style=\"text-align:center\">\n","    <a href=\"https://skills.network/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML311Coursera747-2022-01-01\" target=\"_blank\">\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n","    </a>\n","</p>\n"]},{"cell_type":"markdown","metadata":{"run_control":{"marked":true}},"source":["# Machine Learning Foundation\n","\n","## Course 5, Part g: Transfer Learning DEMO\n"]},{"cell_type":"markdown","metadata":{},"source":["For this exercise, we will use the well-known MNIST digit data. To illustrate the power and concept of transfer learning, we will train a CNN on just the digits 5,6,7,8,9.  Then we will train just the last layer(s) of the network on the digits 0,1,2,3,4 and see how well the features learned on 5-9 help with classifying 0-4.\n","\n","Adapted from https://github.com/fchollet/keras/blob/master/examples/mnist_transfer_cnn.py\n"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["import datetime\n","import keras\n","from keras.datasets import mnist\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Activation, Flatten\n","from keras.layers import Conv2D, MaxPooling2D\n","from keras import backend as K\n","#from tensorflow import keras\n","#from tensorflow.keras.datasets import mnist\n","#from tensorflow.keras.models import Sequential\n","#from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","#from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","#from tensorflow.keras import backend as K"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["#used to help some of the timing functions\n","now = datetime.datetime.now"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["# set some parameters\n","batch_size = 128\n","num_classes = 5\n","epochs = 5"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["# set some more parameters\n","img_rows, img_cols = 28, 28\n","filters = 32\n","pool_size = 2\n","kernel_size = 3"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["## This just handles some variability in how the input data is loaded\n","\n","if K.image_data_format() == 'channels_first':\n","    input_shape = (1, img_rows, img_cols)\n","else:\n","    input_shape = (img_rows, img_cols, 1)"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["## To simplify things, write a function to include all the training steps\n","## As input, function takes a model, training set, test set, and the number of classes\n","## Inside the model object will be the state about which layers we are freezing and which we are training\n","\n","def train_model(model, train, test, num_classes):\n","    x_train = train[0].reshape((train[0].shape[0],) + input_shape)\n","    x_test = test[0].reshape((test[0].shape[0],) + input_shape)\n","    x_train = x_train.astype('float32')\n","    x_test = x_test.astype('float32')\n","    x_train /= 255\n","    x_test /= 255\n","    print('x_train shape:', x_train.shape)\n","    print(x_train.shape[0], 'train samples')\n","    print(x_test.shape[0], 'test samples')\n","\n","    # convert class vectors to binary class matrices\n","    y_train = keras.utils.to_categorical(train[1], num_classes)\n","    y_test = keras.utils.to_categorical(test[1], num_classes)\n","\n","    model.compile(loss='categorical_crossentropy',\n","                  optimizer='adadelta',\n","                  metrics=['accuracy'])\n","\n","    t = now()\n","    model.fit(x_train, y_train,\n","              batch_size=batch_size,\n","              epochs=epochs,\n","              verbose=1,\n","              validation_data=(x_test, y_test))\n","    print('Training time: %s' % (now() - t))\n","\n","    score = model.evaluate(x_test, y_test, verbose=0)\n","    print('Test score:', score[0])\n","    print('Test accuracy:', score[1])"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["# the data, shuffled and split between train and test sets\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","\n","# create two datasets: one with digits below 5 and one with 5 and above\n","x_train_lt5 = x_train[y_train < 5]\n","y_train_lt5 = y_train[y_train < 5]\n","x_test_lt5 = x_test[y_test < 5]\n","y_test_lt5 = y_test[y_test < 5]\n","\n","x_train_gte5 = x_train[y_train >= 5]\n","y_train_gte5 = y_train[y_train >= 5] - 5\n","x_test_gte5 = x_test[y_test >= 5]\n","y_test_gte5 = y_test[y_test >= 5] - 5"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["# Define the \"feature\" layers.  These are the early layers that we expect will \"transfer\"\n","# to a new problem.  We will freeze these layers during the fine-tuning process\n","\n","feature_layers = [\n","    Conv2D(filters, kernel_size,\n","           padding='valid',\n","           input_shape=input_shape),\n","    Activation('relu'),\n","    Conv2D(filters, kernel_size),\n","    Activation('relu'),\n","    MaxPooling2D(pool_size=pool_size),\n","    Dropout(0.25),\n","    Flatten(),\n","]"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["# Define the \"classification\" layers.  These are the later layers that predict the specific classes from the features\n","# learned by the feature layers.  This is the part of the model that needs to be re-trained for a new problem\n","\n","classification_layers = [\n","    Dense(128),\n","    Activation('relu'),\n","    Dropout(0.5),\n","    Dense(num_classes),\n","    Activation('softmax')\n","]"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["# We create our model by combining the two sets of layers as follows\n","model = Sequential(feature_layers + classification_layers)"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_4 (Conv2D)           (None, 26, 26, 32)        320       \n","                                                                 \n"," activation_8 (Activation)   (None, 26, 26, 32)        0         \n","                                                                 \n"," conv2d_5 (Conv2D)           (None, 24, 24, 32)        9248      \n","                                                                 \n"," activation_9 (Activation)   (None, 24, 24, 32)        0         \n","                                                                 \n"," max_pooling2d_2 (MaxPooling  (None, 12, 12, 32)       0         \n"," 2D)                                                             \n","                                                                 \n"," dropout_4 (Dropout)         (None, 12, 12, 32)        0         \n","                                                                 \n"," flatten_2 (Flatten)         (None, 4608)              0         \n","                                                                 \n"," dense_4 (Dense)             (None, 128)               589952    \n","                                                                 \n"," activation_10 (Activation)  (None, 128)               0         \n","                                                                 \n"," dropout_5 (Dropout)         (None, 128)               0         \n","                                                                 \n"," dense_5 (Dense)             (None, 5)                 645       \n","                                                                 \n"," activation_11 (Activation)  (None, 5)                 0         \n","                                                                 \n","=================================================================\n","Total params: 600,165\n","Trainable params: 600,165\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["# Let's take a look\n","model.summary()"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["x_train shape: (29404, 28, 28, 1)\n","29404 train samples\n","4861 test samples\n","Epoch 1/5\n","230/230 [==============================] - 25s 99ms/step - loss: 1.6087 - accuracy: 0.2103 - val_loss: 1.5915 - val_accuracy: 0.2615\n","Epoch 2/5\n","230/230 [==============================] - 24s 103ms/step - loss: 1.5893 - accuracy: 0.2558 - val_loss: 1.5701 - val_accuracy: 0.3170\n","Epoch 3/5\n","230/230 [==============================] - 24s 102ms/step - loss: 1.5690 - accuracy: 0.3040 - val_loss: 1.5471 - val_accuracy: 0.4059\n","Epoch 4/5\n","230/230 [==============================] - 23s 98ms/step - loss: 1.5462 - accuracy: 0.3598 - val_loss: 1.5219 - val_accuracy: 0.5087\n","Epoch 5/5\n","230/230 [==============================] - 23s 100ms/step - loss: 1.5230 - accuracy: 0.4075 - val_loss: 1.4942 - val_accuracy: 0.6036\n","Training time: 0:01:58.322510\n","Test score: 1.4941926002502441\n","Test accuracy: 0.6035795211791992\n"]}],"source":["# Now, let's train our model on the digits 5,6,7,8,9\n","\n","train_model(model,\n","            (x_train_gte5, y_train_gte5),\n","            (x_test_gte5, y_test_gte5), num_classes)"]},{"cell_type":"markdown","metadata":{},"source":["### Freezing Layers\n","Keras allows layers to be \"frozen\" during the training process.  That is, some layers would have their weights updated during the training process, while others would not.  This is a core part of transfer learning, the ability to train just the last one or several layers.\n","\n","Note also, that a lot of the training time is spent \"back-propagating\" the gradients back to the first layer.  Therefore, if we only need to compute the gradients back a small number of layers, the training time is much quicker per iteration.  This is in addition to the savings gained by being able to train on a smaller data set.\n"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["# Freeze only the feature layers\n","for l in feature_layers:\n","    l.trainable = False"]},{"cell_type":"markdown","metadata":{},"source":["Observe below the differences between the number of *total params*, *trainable params*, and *non-trainable params*.\n"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_4 (Conv2D)           (None, 26, 26, 32)        320       \n","                                                                 \n"," activation_8 (Activation)   (None, 26, 26, 32)        0         \n","                                                                 \n"," conv2d_5 (Conv2D)           (None, 24, 24, 32)        9248      \n","                                                                 \n"," activation_9 (Activation)   (None, 24, 24, 32)        0         \n","                                                                 \n"," max_pooling2d_2 (MaxPooling  (None, 12, 12, 32)       0         \n"," 2D)                                                             \n","                                                                 \n"," dropout_4 (Dropout)         (None, 12, 12, 32)        0         \n","                                                                 \n"," flatten_2 (Flatten)         (None, 4608)              0         \n","                                                                 \n"," dense_4 (Dense)             (None, 128)               589952    \n","                                                                 \n"," activation_10 (Activation)  (None, 128)               0         \n","                                                                 \n"," dropout_5 (Dropout)         (None, 128)               0         \n","                                                                 \n"," dense_5 (Dense)             (None, 5)                 645       \n","                                                                 \n"," activation_11 (Activation)  (None, 5)                 0         \n","                                                                 \n","=================================================================\n","Total params: 600,165\n","Trainable params: 590,597\n","Non-trainable params: 9,568\n","_________________________________________________________________\n"]}],"source":["model.summary()"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["x_train shape: (30596, 28, 28, 1)\n","30596 train samples\n","5139 test samples\n","Epoch 1/5\n","240/240 [==============================] - 10s 36ms/step - loss: 1.5767 - accuracy: 0.3050 - val_loss: 1.5516 - val_accuracy: 0.4147\n","Epoch 2/5\n","240/240 [==============================] - 8s 34ms/step - loss: 1.5398 - accuracy: 0.3666 - val_loss: 1.5121 - val_accuracy: 0.5085\n","Epoch 3/5\n","240/240 [==============================] - 8s 34ms/step - loss: 1.5041 - accuracy: 0.4272 - val_loss: 1.4731 - val_accuracy: 0.6098\n","Epoch 4/5\n","240/240 [==============================] - 9s 39ms/step - loss: 1.4684 - accuracy: 0.4874 - val_loss: 1.4352 - val_accuracy: 0.6826\n","Epoch 5/5\n","240/240 [==============================] - 8s 35ms/step - loss: 1.4336 - accuracy: 0.5444 - val_loss: 1.3978 - val_accuracy: 0.7365\n","Training time: 0:00:43.612068\n","Test score: 1.3978396654129028\n","Test accuracy: 0.7365246415138245\n"]}],"source":["train_model(model,\n","            (x_train_lt5, y_train_lt5),\n","            (x_test_lt5, y_test_lt5), num_classes)"]},{"cell_type":"markdown","metadata":{},"source":["Note that after a single epoch, we are already achieving results on classifying 0-4 that are comparable to those achieved on 5-9 after 5 full epochs.  This despite the fact the we are only \"fine-tuning\" the last layer of the network, and all the early layers have never seen what the digits 0-4 look like.\n","\n","Also, note that even though nearly all (590K/600K) of the *parameters* were trainable, the training time per epoch was still much reduced.  This is because the unfrozen part of the network was very shallow, making backpropagation faster. \n"]},{"cell_type":"markdown","metadata":{},"source":["## Exercise\n","- Now we will write code to reverse this training process.  That is, train on the digits 0-4, then finetune only the last layers on the digits 5-9.\n"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_3\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_6 (Conv2D)           (None, 26, 26, 32)        320       \n","                                                                 \n"," activation_12 (Activation)  (None, 26, 26, 32)        0         \n","                                                                 \n"," conv2d_7 (Conv2D)           (None, 24, 24, 32)        9248      \n","                                                                 \n"," activation_13 (Activation)  (None, 24, 24, 32)        0         \n","                                                                 \n"," max_pooling2d_3 (MaxPooling  (None, 12, 12, 32)       0         \n"," 2D)                                                             \n","                                                                 \n"," dropout_6 (Dropout)         (None, 12, 12, 32)        0         \n","                                                                 \n"," flatten_3 (Flatten)         (None, 4608)              0         \n","                                                                 \n"," dense_6 (Dense)             (None, 128)               589952    \n","                                                                 \n"," activation_14 (Activation)  (None, 128)               0         \n","                                                                 \n"," dropout_7 (Dropout)         (None, 128)               0         \n","                                                                 \n"," dense_7 (Dense)             (None, 5)                 645       \n","                                                                 \n"," activation_15 (Activation)  (None, 5)                 0         \n","                                                                 \n","=================================================================\n","Total params: 600,165\n","Trainable params: 600,165\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["# Create layers and define the model as above\n","feature_layers2 = [\n","    Conv2D(filters, kernel_size,\n","           padding='valid',\n","           input_shape=input_shape),\n","    Activation('relu'),\n","    Conv2D(filters, kernel_size),\n","    Activation('relu'),\n","    MaxPooling2D(pool_size=pool_size),\n","    Dropout(0.25),\n","    Flatten(),\n","]\n","\n","classification_layers2 = [\n","    Dense(128),\n","    Activation('relu'),\n","    Dropout(0.5),\n","    Dense(num_classes),\n","    Activation('softmax')\n","]\n","model2 = Sequential(feature_layers2 + classification_layers2)\n","model2.summary()"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["x_train shape: (30596, 28, 28, 1)\n","30596 train samples\n","5139 test samples\n","Epoch 1/5\n","240/240 [==============================] - 30s 120ms/step - loss: 1.5913 - accuracy: 0.2777 - val_loss: 1.5731 - val_accuracy: 0.4532\n","Epoch 2/5\n","240/240 [==============================] - 24s 102ms/step - loss: 1.5631 - accuracy: 0.3722 - val_loss: 1.5404 - val_accuracy: 0.6511\n","Epoch 3/5\n","240/240 [==============================] - 24s 99ms/step - loss: 1.5305 - accuracy: 0.4674 - val_loss: 1.5016 - val_accuracy: 0.7902\n","Epoch 4/5\n","240/240 [==============================] - 24s 99ms/step - loss: 1.4924 - accuracy: 0.5529 - val_loss: 1.4550 - val_accuracy: 0.8653\n","Epoch 5/5\n","240/240 [==============================] - 26s 107ms/step - loss: 1.4452 - accuracy: 0.6266 - val_loss: 1.3983 - val_accuracy: 0.8978\n","Training time: 0:02:07.424502\n","Test score: 1.3982841968536377\n","Test accuracy: 0.8978400230407715\n"]}],"source":["# Now, let's train our model on the digits 0,1,2,3,4\n","train_model(model2,\n","            (x_train_lt5, y_train_lt5),\n","            (x_test_lt5, y_test_lt5), num_classes)"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[],"source":["#Freeze layers\n","for l in feature_layers2:\n","    l.trainable = False"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_3\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_6 (Conv2D)           (None, 26, 26, 32)        320       \n","                                                                 \n"," activation_12 (Activation)  (None, 26, 26, 32)        0         \n","                                                                 \n"," conv2d_7 (Conv2D)           (None, 24, 24, 32)        9248      \n","                                                                 \n"," activation_13 (Activation)  (None, 24, 24, 32)        0         \n","                                                                 \n"," max_pooling2d_3 (MaxPooling  (None, 12, 12, 32)       0         \n"," 2D)                                                             \n","                                                                 \n"," dropout_6 (Dropout)         (None, 12, 12, 32)        0         \n","                                                                 \n"," flatten_3 (Flatten)         (None, 4608)              0         \n","                                                                 \n"," dense_6 (Dense)             (None, 128)               589952    \n","                                                                 \n"," activation_14 (Activation)  (None, 128)               0         \n","                                                                 \n"," dropout_7 (Dropout)         (None, 128)               0         \n","                                                                 \n"," dense_7 (Dense)             (None, 5)                 645       \n","                                                                 \n"," activation_15 (Activation)  (None, 5)                 0         \n","                                                                 \n","=================================================================\n","Total params: 600,165\n","Trainable params: 590,597\n","Non-trainable params: 9,568\n","_________________________________________________________________\n"]}],"source":["model2.summary()"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["x_train shape: (29404, 28, 28, 1)\n","29404 train samples\n","4861 test samples\n","Epoch 1/5\n","230/230 [==============================] - 9s 36ms/step - loss: 1.6180 - accuracy: 0.2439 - val_loss: 1.5927 - val_accuracy: 0.3693\n","Epoch 2/5\n","230/230 [==============================] - 9s 37ms/step - loss: 1.5891 - accuracy: 0.2822 - val_loss: 1.5611 - val_accuracy: 0.4079\n","Epoch 3/5\n","230/230 [==============================] - 8s 35ms/step - loss: 1.5609 - accuracy: 0.3282 - val_loss: 1.5299 - val_accuracy: 0.4781\n","Epoch 4/5\n","230/230 [==============================] - 8s 35ms/step - loss: 1.5334 - accuracy: 0.3815 - val_loss: 1.4994 - val_accuracy: 0.5869\n","Epoch 5/5\n","230/230 [==============================] - 8s 36ms/step - loss: 1.5043 - accuracy: 0.4428 - val_loss: 1.4696 - val_accuracy: 0.6731\n","Training time: 0:00:42.160421\n","Test score: 1.4695541858673096\n","Test accuracy: 0.6731125116348267\n"]}],"source":["train_model(model2,\n","            (x_train_gte5, y_train_gte5),\n","            (x_test_gte5, y_test_gte5), num_classes)"]},{"cell_type":"markdown","metadata":{},"source":["---\n","### Machine Learning Foundation (C) 2020 IBM Corporation\n"]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"}},"nbformat":4,"nbformat_minor":4}
